@inproceedings{hedeshy2020giuplayer,
	title        = {GIUPlayer: A Gaze Immersive YouTube Player Enabling Eye Control and Attention Analysis},
	author       = {Hedeshy, Ramin and Kumar, Chandan and Menges, Raphael and Staab, Steffen},
	year         = 2020,
	booktitle    = {ACM Symposium on Eye Tracking Research and Applications},
	location     = {Stuttgart, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ETRA '20 Adjunct},
	doi          = {10.1145/3379157.3391984},
	isbn         = 9781450371353,
	url          = {https://doi.org/10.1145/3379157.3391984},
	articleno    = 1,
	numpages     = 3,
	keywords     = {Video player, eye tracking, Web accessibility}
}
@inproceedings{hedeshy2021hummer,
	title        = {Hummer: Text Entry by Gaze and Hum},
	author       = {Hedeshy, Ramin and Kumar, Chandan and Menges, Raphael and Staab, Steffen},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	location     = {Yokohama, Japan},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {CHI '21},
	doi          = {10.1145/3411764.3445501},
	isbn         = 9781450380966,
	url          = {https://doi.org/10.1145/3411764.3445501},
	abstract     = {Text entry by gaze is a useful means of hands-free interaction that is applicable in settings where dictation suffers from poor voice recognition or where spoken words and sentences jeopardize privacy or confidentiality. However, text entry by gaze still shows inferior performance and it quickly exhausts its users. We introduce text entry by gaze and hum as a novel hands-free text entry. We review related literature to converge to word-level text entry by analysis of gaze paths that are temporally constrained by humming. We develop and evaluate two design choices: ``HumHum'' and ``Hummer.'' The first method requires short hums to indicate the start and end of a word. The second method interprets one continuous humming as an indication of the start and end of a word. In an experiment with 12 participants, Hummer achieved a commendable text entry rate of 20.45 words per minute, and outperformed HumHum and the gaze-only method EyeSwipe in both quantitative and qualitative measures.},
	articleno    = 741,
	numpages     = 11,
	keywords     = {humming, eye tracking, eye typing, swipe, hands-free interaction}
}
@article{kumar2016eim,
	title        = {Eye-Controlled Interfaces for Multimedia Interaction},
	author       = {Kumar, Chandan and Menges, Raphael and Staab, Steffen},
	year         = 2016,
	month        = oct,
	journal      = {IEEE MultiMedia},
	publisher    = {IEEE Computer Society Press},
	address      = {Los Alamitos, CA, USA},
	volume       = 23,
	number       = 4,
	pages        = {6--13},
	doi          = {10.1109/MMUL.2016.52},
	issn         = {1070-986X},
	url          = {https://doi.org/10.1109/MMUL.2016.52},
	issue_date   = {October 2016},
	numpages     = 8,
	acmid        = 3024131
}
@inproceedings{kumar2017aug,
	title        = {Assessing the Usability of Gaze-Adapted Interface against Conventional Eye-Based Input Emulation},
	author       = {Kumar, Chandan and Menges, Raphael and Staab, Steffen},
	year         = 2017,
	month        = jun,
	booktitle    = {2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS)},
	publisher    = {IEEE},
	volume       = {Special Track on Multimodal Interfaces for Natural Human Computer Interaction: Theory and Applications},
	pages        = {793--798},
	doi          = {10.1109/CBMS.2017.155},
	keywords     = {gaze tracking;graphical user interfaces;human computer interaction;human factors;keyboards;mouse controllers (computers);social networking (online);graphical user interface;browser interface;gaze-adapted interface usability assessibility;gaze-adapted Twitter application interface;gaze-based mouse emulation;gaze-based keyboard emulation;subjective user experience;improved user experience;eye gaze input;eye tracking systems;Keyboards;Twitter;Mice;Emulation;Usability;Bars;eye tracking;gaze input;eye-controlled interfaces;social networks;user-centered design;usability analysis}
}
@inproceedings{kumar2017cbf,
	title        = {Chromium Based Framework to Include Gaze Interaction in Web Browser},
	author       = {Kumar, Chandan and Menges, Raphael and M\"{u}ller, Daniel and Staab, Steffen},
	year         = 2017,
	booktitle    = {Proceedings of the 26th International Conference on World Wide Web Companion},
	location     = {Perth, Australia},
	publisher    = {International World Wide Web Conferences Steering Committee},
	address      = {Republic and Canton of Geneva, Switzerland},
	series       = {WWW '17 Companion},
	pages        = {219--223},
	doi          = {10.1145/3041021.3054730},
	isbn         = {978-1-4503-4914-7},
	url          = {https://doi.org/10.1145/3041021.3054730},
	numpages     = 5,
	acmid        = 3054730,
	keywords     = {chromium embedded framework, dom node extraction, eye-controlled interfaces, gaze input, interactive elements, web browser, webpage rendering}
}
@inproceedings{kumar2019tgp,
	title        = {TouchGazePath: Multimodal Interaction with Touch and Gaze Path for Secure Yet Efficient PIN Entry},
	author       = {Kumar, Chandan and Akbari, Daniyal and Menges, Raphael and MacKenzie, Scott and Staab, Steffen},
	year         = 2019,
	booktitle    = {2019 International Conference on Multimodal Interaction},
	location     = {Suzhou, China},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	series       = {ICMI '19},
	pages        = {329--338},
	doi          = {10.1145/3340555.3353734},
	isbn         = {978-1-4503-6860-5},
	url          = {http://doi.acm.org/10.1145/3340555.3353734},
	numpages     = 10,
	acmid        = 3353734,
	keywords     = {Authentication, PIN entry, eye tracking, gaze path, multimodal interaction, usable security}
}
@incollection{kumar2020mamem,
	title        = {Eye tracking for Interaction: Evaluation Methods},
	author       = {Kumar, Chandan and Menges, Raphael and Sengupta, Korok and Staab, Steffen},
	year         = 2020,
	booktitle    = {Signal Processing to Drive Human-Computer Interaction: EEG and eye-controlled interfaces},
	publisher    = {Institution of Engineering and Technology},
	series       = {Healthcare Technologies, Institution of Engineering and Technology},
	pages        = {117--144},
	doi          = {10.1049/PBCE129E_ch6},
	isbn         = 9781785619199,
	url          = {https://digital-library.theiet.org/content/books/10.1049/pbce129e_ch6},
	city         = {London},
	country      = {United Kingdom},
	editor       = {Nikolopoulos, Spiros and Kumar, Chandan and Kompatsiaris, Ioannis},
	chapter      = 6
}
@article{lichtenberg2018ars,
	title        = {Analyzing Residue Surface Proximity to Interpret Molecular Dynamics},
	author       = {Lichtenberg, Nils and Menges, Raphael and Ageev, Vladimir and George, Ajay Abisheck Paul and Heimer, Pascal and Imhof, Diana and Lawonn, Kai},
	year         = 2018,
	journal      = {Computer Graphics Forum},
	publisher    = {The Eurographics Association and John Wiley & Sons Ltd.},
	doi          = {10.1111/cgf.13427},
	issn         = {1467-8659}
}
@inproceedings{menges2016enf,
	title        = {eyeGUI: A Novel Framework for Eye-Controlled User Interfaces},
	author       = {Menges, Raphael and Kumar, Chandan and Sengupta, Korok and Staab, Steffen},
	year         = 2016,
	booktitle    = {Proceedings of the 9th Nordic Conference on Human-Computer Interaction},
	location     = {Gothenburg, Sweden},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	series       = {NordiCHI '16},
	pages        = {121:1--121:6},
	doi          = {10.1145/2971485.2996756},
	isbn         = {978-1-4503-4763-1},
	url          = {http://doi.acm.org/10.1145/2971485.2996756},
	articleno    = 121,
	numpages     = 6,
	acmid        = 2996756,
	keywords     = {Gaze input, eye tracking, eye-controlled interfaces, interactive elements, visual feedback}
}
@inproceedings{menges2017ggw,
	title        = {GazeTheWeb: A Gaze-Controlled Web Browser},
	author       = {Menges, Raphael and Kumar, Chandan and M\"{u}ller, Daniel and Sengupta, Korok},
	year         = 2017,
	booktitle    = {Proceedings of the 14th Web for All Conference on The Future of Accessible Work},
	location     = {Perth, Western Australia, Australia},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	series       = {W4A '17},
	pages        = {25:1--25:2},
	doi          = {10.1145/3058555.3058582},
	isbn         = {978-1-4503-4900-0},
	url          = {http://doi.acm.org/10.1145/3058555.3058582},
	articleno    = 25,
	numpages     = 2,
	acmid        = 3058582,
	keywords     = {Web accessibility, Web browser, eye tracking, eye-controlled interfaces, gaze input, navigation}
}
@inproceedings{menges2017sgc,
	title        = {Schau genau! A Gaze-Controlled 3D Game for Entertainment and Education},
	author       = {Menges, Raphael and Kumar, Chandan and Wechselberger, Ulrich and Schaefer, Christoph and Walber, Tina and Staab, Steffen},
	year         = 2017,
	booktitle    = {Journal of Eye Movement Research},
	volume       = 10,
	number       = 6,
	pages        = 220,
	issn         = {1995-8692},
	eventdate    = {21. August 2017},
	eventtitle   = {The 2017 COGAIN Symposium},
	venue        = {Wuppertal}
}
@inproceedings{menges2018erw,
	title        = {Enhanced Representation of Web Pages for Usability Analysis with Eye Tracking},
	author       = {Menges, Raphael and Tamimi, Hanadi and Kumar, Chandan and Walber, Tina and Schaefer, Christoph and Staab, Steffen},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \& Applications},
	location     = {Warsaw, Poland},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	series       = {ETRA '18},
	pages        = {18:1--18:9},
	doi          = {10.1145/3204493.3204535},
	isbn         = {978-1-4503-5706-7},
	url          = {http://doi.acm.org/10.1145/3204493.3204535},
	articleno    = 18,
	numpages     = 9,
	acmid        = 3204535,
	keywords     = {eye tracking, viewport-relative elements, web page usability},
	abstract     = {Eye tracking as a tool to quantify user attention plays a major role in research and application design. For Web page usability, it has become a prominent measure to assess which sections of a Web page are read, glanced or skipped. Such assessments primarily depend on the mapping of gaze data to a Web page representation. However, current representation methods, a virtual screenshot of the Web page or a video recording of the complete interaction session, suffer either from accuracy or scalability issues. We present a method that identifies fixed elements on Web pages and combines user viewport screenshots in relation to fixed elements for an enhanced representation of the page. We conducted an experiment with 10 participants and the results signify that analysis with our method is more efficient than a video recording, which is an essential criterion for large scale Web studies.}
}
@article{menges2019iue,
	title        = {Improving User Experience of Eye Tracking-Based Interaction: Introspecting and Adapting Interfaces},
	author       = {Menges, Raphael and Kumar, Chandan and Staab, Steffen},
	year         = 2019,
	month        = nov,
	journal      = {ACM Trans. Comput.-Hum. Interact.},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	volume       = 26,
	number       = 6,
	pages        = {37:1--37:46},
	doi          = {10.1145/3338844},
	issn         = {1073-0516},
	url          = {http://doi.acm.org/10.1145/3338844},
	issue_date   = {November 2019},
	articleno    = 37,
	numpages     = 46,
	acmid        = 3338844,
	keywords     = {Eye tracking, GazeTheWeb, Web accessibility, gaze interaction experience, gaze-based emulation, gaze-controlled interface, interface semantics, introspection},
	abstract     = {Eye tracking systems have greatly improved in recent years, being a viable and affordable option as digital communication channel, especially for people lacking fine motor skills. Using eye tracking as an input method is challenging due to accuracy and ambiguity issues, and therefore research in eye gaze interaction is mainly focused on better pointing and typing methods. However, these methods eventually need to be assimilated to enable users to control application interfaces. A common approach to employ eye tracking for controlling application interfaces is to emulate mouse and keyboard functionality. We argue that the emulation approach incurs unnecessary interaction and visual overhead for users, aggravating the entire experience of gaze-based computer access. We discuss how the knowledge about the interface semantics can help reducing the interaction and visual overhead to improve the user experience. Thus, we propose the efficient introspection of interfaces to retrieve the interface semantics and adapt the interaction with eye gaze. We have developed a Web browser, GazeTheWeb, that introspects Web page interfaces and adapts both the browser interface and the interaction elements on Web pages for gaze input. In a summative lab study with 20 participants, GazeTheWeb allowed the participants to accomplish information search and browsing tasks significantly faster than an emulation approach. Additional feasibility tests of GazeTheWeb in lab and home environment showcase its effectiveness in accomplishing daily Web browsing activities and adapting large variety of modern Web pages to suffice the interaction for people with motor impairment.}
}
@incollection{menges2020mamem,
	title        = {Eye tracking for Interaction: Adapting Multimedia Interfaces},
	author       = {Menges, Raphael and Kumar, Chandan and Staab, Steffen},
	year         = 2020,
	booktitle    = {Signal Processing to Drive Human-Computer Interaction: EEG and eye-controlled interfaces},
	publisher    = {Institution of Engineering and Technology},
	series       = {Healthcare Technologies, Institution of Engineering and Technology},
	pages        = {83--116},
	doi          = {10.1049/PBCE129E_ch5},
	isbn         = 9781785619199,
	url          = {https://digital-library.theiet.org/content/books/10.1049/pbce129e_ch5},
	city         = {London},
	country      = {United Kingdom},
	editor       = {Nikolopoulos, Spiros and Kumar, Chandan and Kompatsiaris, Ioannis},
	chapter      = 5
}
@inproceedings{menges2020vistool,
	title        = {A Visualization Tool for Eye Tracking Data Analysis in the Web},
	author       = {Menges, Raphael and Kramer, Sophia and Hill, Stefan and Nisslmueller, Marius and Kumar, Chandan and Staab, Steffen},
	year         = 2020,
	booktitle    = {ACM Symposium on Eye Tracking Research and Applications},
	location     = {Stuttgart, Germany},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ETRA '20 Short Papers},
	doi          = {10.1145/3379156.3391831},
	isbn         = 9781450371346,
	url          = {https://doi.org/10.1145/3379156.3391831},
	articleno    = 46,
	numpages     = 5,
	keywords     = {Web interaction, gaze visualization, heatmap, scanpath}
}
@article{schaefer2014sga,
	title        = {Schau genau! - an Eye Tracking Game With a Purpose},
	author       = {Schaefer, Christoph and Kuich, Matthias and Menges, Raphael and Schmidt, Kevin and Walber, Tina},
	year         = 2014,
	booktitle    = {1st.  Workshop  on  the  Applications  for  Gaze  in Games at CHI Play 2014},
	address      = {Toronto, Canada}
}
@inproceedings{sengupta2017aic,
	title        = {Analyzing the Impact of Cognitive Load in Evaluating Gaze-Based Typing},
	author       = {Sengupta, Korok and Sun, Jun and Menges, Raphael and Kumar, Chandan and Staab, Steffen},
	year         = 2017,
	month        = jun,
	booktitle    = {2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS)},
	publisher    = {IEEE},
	volume       = {Special Track on Multimodal Interfaces for Natural Human Computer Interaction: Theory and Applications},
	pages        = {787--792},
	doi          = {10.1109/CBMS.2017.134},
	keywords     = {cognition;electroencephalography;eye;Fourier transforms;gaze tracking;human computer interaction;keyboards;medical signal processing;neurophysiology;user interfaces;text entry performance measures;Gaze-based virtual keyboards;keyboard usability;Short-time Fourier Transform;EEG signal analysis;eye typing usability;word suggestion positioning;eye typing evaluation measure;eye gaze;human brain cognition;natural eye movements;cognitive load;Keyboards;Electroencephalography;Visualization;Layout;eye typing;gaze input;EEG;cognitive load}
}
@inproceedings{sengupta2017gik,
	title        = {GazeTheKey: Interactive Keys to Integrate Word Predictions for Gaze-based Text Entry},
	author       = {Sengupta, Korok and Menges, Raphael and Kumar, Chandan and Staab, Steffen},
	year         = 2017,
	booktitle    = {Proceedings of the 22Nd International Conference on Intelligent User Interfaces Companion},
	location     = {Limassol, Cyprus},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	series       = {IUI '17 Companion},
	pages        = {121--124},
	doi          = {10.1145/3030024.3038259},
	isbn         = {978-1-4503-4893-5},
	url          = {http://doi.acm.org/10.1145/3030024.3038259},
	numpages     = 4,
	acmid        = 3038259,
	keywords     = {dwell time, eye tracking, eye typing, gaze input, text entry, visual feedback}
}
@inproceedings{sengupta2018hwb,
	title        = {Hands-free Web Browsing: Enriching the User Experience with Gaze and Voice Modality},
	author       = {Sengupta, Korok and Ke, Min and Menges, Raphael and Kumar, Chandan and Staab, Steffen},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research \& Applications},
	location     = {Warsaw, Poland},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	series       = {ETRA '18},
	pages        = {88:1--88:3},
	doi          = {10.1145/3204493.3208338},
	isbn         = {978-1-4503-5706-7},
	url          = {http://doi.acm.org/10.1145/3204493.3208338},
	articleno    = 88,
	numpages     = 3,
	acmid        = 3208338,
	keywords     = {eye tracking, hands-free interaction, multimodal interfaces, speech commands, voice input, web accessibility}
}
@inproceedings{sengupta2019ivp,
	title        = {Impact of Variable Positioning of Text Prediction in Gaze-based Text Entry},
	author       = {Sengupta, Korok and Menges, Raphael and Kumar, Chandan and Staab, Steffen},
	year         = 2019,
	booktitle    = {Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
	location     = {Denver, Colorado},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	series       = {ETRA '19},
	pages        = {74:1--74:9},
	doi          = {10.1145/3317956.3318152},
	isbn         = {978-1-4503-6709-7},
	url          = {http://doi.acm.org/10.1145/3317956.3318152},
	articleno    = 74,
	numpages     = 9,
	acmid        = 3318152,
	keywords     = {gaze input, interaction, text entry, text prediction, variable position}
}
@inproceedings{hedeshy2023cnvve,
	title        = {CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice},
	author       = {Hedeshy, Ramin and Menges, Raphael and Staab, Steffen},
	year         = 2023,
	booktitle    = {Proceedings of INTERSPEECH 2023},
	pages        = {1553--1557},
	doi          = {10.21437/Interspeech.2023-201}
}